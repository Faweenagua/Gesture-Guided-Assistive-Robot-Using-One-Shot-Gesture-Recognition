# Gesture Guided Assistive Robot Using One-Shot Gesture Recognition
Assistive robots aim to improve human independence by supporting tasks such as mobility assistance, object handling, and social interaction. 
In this work, we present an autonomous human-following assistive robot that interprets user intent through gesture-based commands and adapts its behavior in real time within indoor environments. 
The system combines a webcam-based pose-estimation module for spatial perception with a One-Shot Gesture Recognition (OSG) framework capable of learning and recognizing gestures from a single demonstration. 
This reduces the need for large datasets and eliminates the requirement for user-specific retraining. 
The full pipeline is implemented on a TurtleBot3 Burger platform, where we validate gesture-driven navigation and human-following behavior in a real-world setting. 
The results show that lightweight, one-shot gesture learning can enable intuitive and customizable interaction for assistive robotic systems.

This project uses an OSG (One-Shot Gesture) model implemented here https://github.com/IRVLab/One-Shot-Gesture-Recognition
